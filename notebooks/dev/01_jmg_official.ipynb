{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official data\n",
    "\n",
    "This repo loads and analyses official data for the FNF project. As part of this, we will:\n",
    "\n",
    "1. Load BRES and IDBR data from Nesta data_getters\n",
    "2. Explore the data focusing on journalism and media\n",
    "  * As part of this we need to select relevant SIC codes\n",
    "3. Produce some initial outputs to be presented at the team meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lq(X, binary=False):\n",
    "    \"\"\" Calculate the location quotient.\n",
    "\n",
    "    Divides the share of activity in a location by the share of activity in the UK total\n",
    "\n",
    "    Args:\n",
    "        X (pandas.DataFrame): DataFrame where rows are locations, columns are sectors and values are activity in a given sector at a location.\n",
    "        binary (bool, optional): If True, discretise the data with a cut-off value of 1\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "    \"\"\"\n",
    "    Xm = X.values\n",
    "    X = pd.DataFrame((Xm/Xm.sum(1)[:, np.newaxis])/(Xm.sum(0)/Xm.sum()),\n",
    "            index=X.index, columns=X.columns)\n",
    "    \n",
    "    return (X > 1) if binary else X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sectors names and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journalism_sics = {'6010':'radio_broadcasting',\n",
    "                   '6020':'tv_programming_broadcasting',\n",
    "                   '1811':'printing_newspapers',\n",
    "                   '5813':'publishing_newspapers',\n",
    "                  '6391':'news_agency_activities',\n",
    "                   '6312':'web_portals',}\n",
    "\n",
    "benchmark = {'62':'computer_programming','73':'advertising',\n",
    "             #'68':'real_estate'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some colour lookups for different sectors.\n",
    "#The idea is to have similar hues for similar sectors, and lower opacity for non-journalism sectors\n",
    "\n",
    "colors = {x:y for x,y in zip(journalism_sics.values(),['lightcoral','red','deepskyblue','blue','cadetblue','darkorchid'])}\n",
    "\n",
    "colors['advertising'] = 'green'\n",
    "colors['journalism'] = 'blue'\n",
    "colors['computer_programming'] = 'red'\n",
    "colors['other'] = 'grey'\n",
    "colors['real_estate'] = 'brown'\n",
    "\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "colors_rgb = {x:to_rgba(y) for x,y in colors.items()}\n",
    "\n",
    "journ_sects = list(journalism_sics.values())+['journalism']\n",
    "\n",
    "colors_rgb = {x:y if x in journ_sects else tuple(list(y[:3])+list((0.8,))) for x,y in colors_rgb.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load geo metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_shape = gp.read_file('../../data/external/lads/Local_Authority_Districts_December_2017_Full_Clipped_Boundaries_in_Great_Britain.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A couple of the LAD names are spelt differently. Let's fix them\n",
    "lad_rename_lookup = {'Rhondda Cynon Taf':'Rhondda Cynon Taff',\n",
    "                 'Shepway':'Folkestone and Hythe'}\n",
    "\n",
    "\n",
    "lad_shape['lad17nm'] = [lad_rename_lookup[x] if x in lad_rename_lookup.keys() else x for x in lad_shape['lad17nm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bres = pd.read_csv('/Users/jmateosgarcia/Desktop/mapping_innovation_scotland/data/interim/BRES_LAD_combi_5_9_2019.csv')\n",
    "idbr = pd.read_csv('/Users/jmateosgarcia/Desktop/mapping_innovation_scotland/data/interim/IDBR_LAD_141_5_9_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bres.columns = [x.strip() for x in bres.columns]\n",
    "idbr.columns = [x.strip() for x in idbr.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_official(df,lookup,benchmark,id_vars=['LAD','year']):\n",
    "    '''\n",
    "    This function processes official data for our analysis of the state and evolution of the journalism industry in the UK\n",
    "    \n",
    "    Arguments:\n",
    "        df is a df with a year and LAD variables we can use to group our analysis\n",
    "        lookup is a sic to names lookup that we use to identify journalism related sectors\n",
    "        benchmark is a lookup we use to label benchmark 2-digit sectors like 60 (computing), 64 (finance) or 68 (real estate)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Melt the data\n",
    "    \n",
    "    melted = pd.melt(df,id_vars=id_vars,var_name='sic_4')\n",
    "    \n",
    "    #print(melted.head())\n",
    "    \n",
    "    #Name each sector\n",
    "    \n",
    "    #This considers each component of journalism separately\n",
    "    melted['sector'] = [lookup[x] if x in lookup.keys() else benchmark[x[:2]] if x[:2] in benchmark.keys() else 'other' for x in melted['sic_4']]\n",
    "    \n",
    "    #This aggregates them\n",
    "    melted['sector_aggregated'] = ['journalism' if x in lookup.keys() else benchmark[x[:2]] if x[:2] in benchmark.keys() else 'other' for x in melted['sic_4']]\n",
    "    \n",
    "    return(melted)\n",
    "\n",
    "def make_trend(df,sector_var,y='year',v='value'):\n",
    "    '''\n",
    "    \n",
    "    Pandas contortions to create a linechart from the previous output\n",
    "    \n",
    "    Args:\n",
    "        df is a long table where every row is a LAD-sector pair\n",
    "        sector_var is the variable we want to visualise\n",
    "        y is the year variable\n",
    "        v is the value\n",
    "    \n",
    "    '''\n",
    "\n",
    "    out = df.groupby([y,sector_var])[v].sum().reset_index(drop=False).pivot(index=y,columns=sector_var,\n",
    "                                                                             values=v)\n",
    "    \n",
    "    return(out)\n",
    "    \n",
    "    \n",
    "def plot_trend(df,ax,norm=True,smooth=3):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Plots trends\n",
    "    \n",
    "    Args:\n",
    "        df is a table where the rows are years and the columns are sectors\n",
    "        ax is the axis for plotting\n",
    "        norm is whether we want to normalise the data by year\n",
    "        rolling is to smooth\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if norm==True:\n",
    "        \n",
    "        for c in df.columns:\n",
    "            (100*df[c]/df[c].sum()).rolling(window=smooth).mean().dropna().plot(ax=ax,linewidth=3 if c in journ_sects else 1,\n",
    "                                                                                              color=colors_rgb[c])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for c in df.columns:\n",
    "            (100*df[c].rolling(window=smooth).mean(),dropna()).plot(ax=ax,linewidth=3 if c in journ_sects else 1,\n",
    "                                                                                              color=colors_rgb[c])\n",
    "        \n",
    "        \n",
    "def plot_bar(df,ax,norm=False,smooth=2,journ=list(journalism_sics.values())):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        df is a table where the rows are years and the columns are sectors\n",
    "        ax is the ax for plotting\n",
    "        norm is whether we normalise the values to share in the year or keep totals\n",
    "        rolling is the window for smoothing\n",
    "        jour are the columsn with journalism data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    journ_df = df[journ]\n",
    " \n",
    "    \n",
    "    sectors_sorted = journ_df.T.sort_values(2017,ascending=False).index    \n",
    "    \n",
    "    if norm==True:\n",
    "        (100*journ_df[sectors_sorted].apply(lambda x: x/x.sum(),axis=1).rolling(window=smooth).mean()).dropna().plot.bar(ax=ax,stacked=True,width=0.8,\n",
    "                                                                                                                        color=[colors[x] for x in sectors_sorted])\n",
    "                \n",
    "    else:\n",
    "        (100*journ_df[sectors_sorted].rolling(window=smooth).mean()).dropna().plot.bar(ax=ax,stacked=True,width=0.8,color=[colors[x] for x in sectors_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lorenz(df,variable='sector',y=2017):\n",
    "    '''\n",
    "    Creates a lorenz curve of concentration\n",
    "    \n",
    "    Args:\n",
    "        df is the same df as before\n",
    "        y is the year to focus on\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    focus = df.loc[df['year']==y].reset_index(drop=False).pivot_table(index='LAD',columns=variable,values='value',aggfunc='sum')\n",
    "    \n",
    "    #For each column we sort and normalise\n",
    "    \n",
    "    cumulatives = []\n",
    "    \n",
    "    for c in focus.columns:\n",
    "        \n",
    "        cumulatives.append(focus[c].sort_values(ascending=False)/focus[c].sum())\n",
    "        \n",
    "        \n",
    "    out = pd.concat(cumulatives,axis=1)\n",
    "    out.columns = focus.columns\n",
    "    \n",
    "        \n",
    "    return(out)\n",
    "    \n",
    "def plot_lorenz(df,ax):\n",
    "    '''\n",
    "    Plots the lorenz curve (share of activity accounted by locations in various positions of the ranking)\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sorted_cont = []\n",
    "    \n",
    "    for c in df.columns:\n",
    "        \n",
    "        sorted_cont.append(df[c].sort_values(ascending=False).cumsum().reset_index(drop=True))\n",
    "        \n",
    "    sorted_cont = pd.concat(sorted_cont,axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sorted_cont.columns = df.columns\n",
    "    \n",
    "    sorted_sectors = sorted_cont.loc[5].sort_values().index[::-1]\n",
    "    \n",
    "    for x in sorted_sectors:\n",
    "        (100*sorted_cont[x]).plot(ax=ax,alpha=0.75,color=colors_rgb[x],\n",
    "                               linewidth=3 if x in journ_sects else 1)\n",
    "        \n",
    "\n",
    "def plot__histo_lorenz(df_1,df_2):\n",
    "    '''\n",
    "    Plots the lorenz curve (share of activity accounted by locations in various positions of the ranking) comparing t1 and t2\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(16,5),ncols=len(df_1.columns),sharey=True)\n",
    "    \n",
    "    sectors = df_2.iloc[10].sort_values().index\n",
    "    \n",
    "    sorted_cont = []\n",
    "    \n",
    "    for n,c in enumerate(sectors):\n",
    "        \n",
    "        df_1[c].sort_values(ascending=False).cumsum().reset_index(drop=True).plot(ax=ax[n],linewidth=2,color='red',alpha=0.75)\n",
    "        df_2[c].sort_values(ascending=False).cumsum().reset_index(drop=True).plot(ax=ax[n],linewidth=2,color='blue',alpha=0.75)\n",
    "        \n",
    "        \n",
    "        ax[n].set_title(re.sub('_','\\n',c),size=10)\n",
    "        \n",
    "        ax[n].legend(labels=['2009','2017'])\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def widen_proc(df,year,variable,normalize=False,make_geo=False,geo_option=lad_shape):\n",
    "    '''\n",
    "    Function that takes a processsed df (as above and widens it). We can then concatenate it with the shapefile for plotting\n",
    "    \n",
    "    Args:\n",
    "        df is the dataframe\n",
    "        year is the year to focus on. \n",
    "        variable is the variable we want in the columns of the df\n",
    "        normalize is whether we want to output LQs or totals\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #We widen a sector df so that every column is a sector \n",
    "    \n",
    "    wide = pd.pivot_table(df.loc[df['year']==year],index='LAD',columns=variable,values='value',aggfunc='sum')\n",
    "    \n",
    "    if normalize == True:\n",
    "        wide = create_lq(wide)\n",
    "        \n",
    "    if make_geo!=False:\n",
    "        \n",
    "        #Merges the sector df with the shapefile\n",
    "        wide =geo_option.merge(wide.reset_index(drop=False),left_on='lad17nm',right_on='LAD')\n",
    "        \n",
    "    return(wide)\n",
    "    \n",
    "\n",
    "def map_sector(geodf,sector,title,ax,**kwargs):\n",
    "    '''\n",
    "    Map a sector\n",
    "    \n",
    "    Args:\n",
    "        geodf is the geo_df\n",
    "        sector is the sector\n",
    "        ax is the ax\n",
    "        kwargs are to control the plot\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    geodf.plot(column=sector,ax=ax,**kwargs)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    \n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "def year_comp(act_df,variable,sector,ax,years=[2009,2017],**kwargs):\n",
    "    '''\n",
    "    \n",
    "    Plots two maps comparing years\n",
    "    \n",
    "    Args:\n",
    "        act df is the df to compare\n",
    "        sector is the sector\n",
    "        years are the years to compare\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #Extract the two years\n",
    "    y1,y2 = [widen_proc(act_df,year=y,variable=variable,normalize=True,make_geo=True) for y in years]\n",
    "\n",
    "    #fig,ax = plt.subplots(figsize=(7,10))\n",
    "\n",
    "    map_sector(y1,sector,title='_'.join([sector,str(years[0])]),ax=ax[0],**kwargs)\n",
    "    \n",
    "    map_sector(y2,sector,title='_'.join([sector,str(years[1])]),ax=ax[1],**kwargs)\n",
    "\n",
    "def sect_comp(act_df,variable,sectors,ax,year=2017,**kwargs):\n",
    "    '''\n",
    "    \n",
    "    Compares various sectors\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Extract the two years\n",
    "    activity = widen_proc(act_df,year=year,variable=variable,normalize=True,make_geo=True)\n",
    "\n",
    "\n",
    "    for n,sector in enumerate(sectors):\n",
    "        map_sector(activity,sector,title=sector,ax=ax[n],**kwargs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sectoral_report(df,plot_destination,year_lims = [2009,2017],proc_kwargs={'lookup':journalism_sics,\n",
    "                                                                             'benchmark':benchmark},\n",
    "                    map_kwargs={'scheme':'Fisher_Jenks','cmap':'viridis','edgecolor':'grey','linewidth':0,'legend':True}):\n",
    "    '''\n",
    "    \n",
    "    Performs a series of routine analyses of official data for the FNF project.\n",
    "    \n",
    "    Args:\n",
    "        plot_destination (str): location to store the plots\n",
    "        year_lims (list) years for historical comparisons\n",
    "        proc_kwargs (dict) parameters to process the data\n",
    "        map_kwargs (dict) parameters to visualise the data in maps\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Process data\n",
    "    proc = process_official(df,**proc_kwargs)\n",
    "    \n",
    "    ##############\n",
    "    ##Trend charts\n",
    "    ##############\n",
    "    print('making trends')\n",
    "    \n",
    "    #Create trend charts\n",
    "    trend_micro,trend_agg = [make_trend(proc,sector_var=v) for v in ['sector','sector_aggregated']]\n",
    "    \n",
    "    #Plot aggregated trends\n",
    "    fig,ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "    plot_trend(trend_agg,ax=ax)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1,1))\n",
    "    ax.set_title('Aggregate trends')\n",
    "    ax.set_ylabel('% in year')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_aggregated_trends.pdf')\n",
    "    \n",
    "    \n",
    "    #Plot micro trends\n",
    "    fig,ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "    plot_trend(trend_micro,ax=ax)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1,1))\n",
    "    ax.set_title('Detailed trends')\n",
    "    ax.set_ylabel('% in year')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_micro_trends.pdf')\n",
    "    \n",
    "    \n",
    "    #Plot bars\n",
    "    fig,ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "    plot_bar(trend_micro,ax,norm=True)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1,1))\n",
    "    ax.set_title('Composition of journalism')\n",
    "    ax.set_ylabel('% in year')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_micro_bar.pdf')\n",
    "    \n",
    "    \n",
    "    ###############\n",
    "    #Lorenz charts\n",
    "    ###############\n",
    "    print('making concentration')\n",
    "    \n",
    "    \n",
    "    y0 = year_lims[0]\n",
    "    y1 = year_lims[1]\n",
    "    \n",
    "    shares_early,shares_late = [make_lorenz(proc,y=year) for year in [y0,y1]]\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(9,6),sharey=True)\n",
    "\n",
    "    plot_lorenz(shares_late,ax)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1,1))\n",
    "    ax.set_title('Concentration by sector')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_lorenz_last.pdf')\n",
    "    \n",
    "    plot__histo_lorenz(shares_early,shares_late)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_lorenz_change.pdf')\n",
    "    \n",
    "    ###########\n",
    "    #Maps\n",
    "    ###########\n",
    "    print('making maps')\n",
    "    \n",
    "    #Map of journalism\n",
    "    fig,ax = plt.subplots(figsize=(14,10),ncols=2)\n",
    "\n",
    "    year_comp(proc,'sector_aggregated','journalism',years=year_lims,ax=ax,**plot_kwargs)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_map_year_comp.png')\n",
    "\n",
    "\n",
    "    \n",
    "    #Map comparing journalism with other sectors\n",
    "    fig,ax = plt.subplots(figsize=(21,10),ncols=3)\n",
    "\n",
    "    sect_comp(proc,'sector_aggregated',['advertising','computer_programming','journalism'],ax=ax,**plot_kwargs)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_map_sector_comp_agg.png')\n",
    "    \n",
    "    #Map comparing newspaper publishing and web portals\n",
    "    fig,ax = plt.subplots(figsize=(14,10),ncols=2)\n",
    "\n",
    "    sect_comp(proc,'sector',['publishing_newspapers','web_portals'],ax=ax,**plot_kwargs)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_map_newspapers_portals.png')\n",
    "    \n",
    "    #Map comparing radio and TV broadcasting\n",
    "    fig,ax = plt.subplots(figsize=(14,10),ncols=2)\n",
    "\n",
    "    sect_comp(proc,'sector',['tv_programming_broadcasting','radio_broadcasting'],ax=ax,**plot_kwargs)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_map_tv_radio.png')\n",
    "    \n",
    "    ###########\n",
    "    #Heatmap\n",
    "    ###########\n",
    "    print('making colocation')\n",
    "    \n",
    "    sector_spec = widen_proc(proc,2017,'sector',normalize=True)\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(9,7))\n",
    "    \n",
    "    sns.heatmap(drop_diagonal(sector_spec.corr()),cmap='bwr',center=0)\n",
    "    ax.set_title('Sector colocation')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_sector_colocation.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_diagonal(corr):\n",
    "    '''\n",
    "    Utility to drop diagonal in a correlation matrix so we can visualise it as a heatmap\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sector_corr_array = np.array(corr)\n",
    "\n",
    "    np.fill_diagonal(sector_corr_array,np.nan)\n",
    "\n",
    "    out = pd.DataFrame(sector_corr_array,index=corr.index,columns=corr.columns)\n",
    "\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Generate report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BRES report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwargs = {'scheme':'Fisher_Jenks','cmap':'viridis','edgecolor':'grey','linewidth':0,'legend':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectoral_report(bres,plot_destination='../../reports/figures/research_slides/bres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A couple of additional statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bres_proc = process_official(bres,journalism_sics,benchmark)\n",
    "bres_proc.groupby(['year','sector_aggregated'])['value'].sum().reset_index(drop=False).pivot(index='year',columns='sector_aggregated',values='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bres_proc.groupby(['year','sector'])['value'].sum().reset_index(drop=False).pivot(index='year',columns='sector',values='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_lorenz(bres_proc)['publishing_newspapers'].sort_values(ascending=False).iloc[:10].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_lorenz(bres_proc)['other'].sort_values(ascending=False).iloc[:10].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_lorenz(bres_proc)['web_portals'].sort_values(ascending=False).iloc[:10].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "57/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDBR report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectoral_report(idbr,plot_destination='../../reports/figures/research_slides/idbr',year_lims=[2010,2017])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with secondary data\n",
    "\n",
    "We combine industrial data reported above with secondary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary = pd.read_csv('../../data/processed/13_9_2019_secondary_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondary_comp(secondary,df,plot_destination,proc_kwargs,activity_threshold=0.25):\n",
    "    '''\n",
    "    \n",
    "    Here we combine secondary LAD-level data with industry data and analyse their correlations\n",
    "    \n",
    "    Args:\n",
    "        secondary is a df with secondary data\n",
    "        df is a df with industry data (could be bres or idbr)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Calculate sector specialisation\n",
    "    \n",
    "    print('making clustering')\n",
    "    \n",
    "    #First we need to process the data\n",
    "    proc = process_official(df,**proc_kwargs)\n",
    "    \n",
    "    #Then calculate specialisations\n",
    "    sector_spec = widen_proc(proc,2017,'sector',normalize=True)\n",
    "    \n",
    "    #Merge with the secondary data\n",
    "    combined = pd.concat([sector_spec,secondary],axis=1)\n",
    "    \n",
    "    #Plot clustermap\n",
    "    #fig,ax = plt.subplots(12,12)\n",
    "    \n",
    "    sns.clustermap(combined.corr(),cmap='bwr',figsize=(6,6)\n",
    "                   #ax=ax\n",
    "                  )\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_secondary_clustering.pdf')\n",
    "    \n",
    "    \n",
    "    #Identify LADs with low levels of journalism employment\n",
    "    \n",
    "    print('checking low activity areas')\n",
    "    journalism_spec = widen_proc(proc,2017,'sector_aggregated',normalize=False)['journalism']\n",
    "    \n",
    "    #Low quantile\n",
    "    quant = journalism_spec.quantile(activity_threshold)\n",
    "    \n",
    "    print(quant)\n",
    "    \n",
    "    #Create a dummy if value below quantile\n",
    "    journalism_low = journalism_spec<quant\n",
    "    \n",
    "    #Add this to the data\n",
    "    sec_j = pd.concat([sec,journalism_low],axis=1)\n",
    "\n",
    "    #Compare high w/ low areas\n",
    "    comparison = pd.concat([sec_j.groupby('journalism')[x].median() for x in secondary.columns],axis=1).iloc[:,2:]\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(8,5))\n",
    "    \n",
    "    (100*(comparison.loc[True]/comparison.loc[False])-100).sort_values(ascending=True).plot.barh(color='blue',ax=ax)\n",
    "    \n",
    "    ax.set_title('Profile: areas with low journalism density')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'../../data/processed/{plot_destination}/{today_str}_profile_comparison.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_comp(secondary,bres,'../../reports/figures/research_slides/bres/',proc_kwargs={'lookup':journalism_sics,\n",
    "                                                                             'benchmark':benchmark})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journalism_sics_filter = journalism_sics.copy()\n",
    "\n",
    "del journalism_sics_filter['1811']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_comp(secondary,idbr,'../../reports/figures/research_slides/idbr/',proc_kwargs={'lookup':journalism_sics_filter,\n",
    "                                                                             'benchmark':benchmark},activity_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure number of journalists per capita\n",
    "\n",
    "We use longitudinal population data from Nomis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad_pop = pd.read_csv('../../data/processed/13_9_2019_lad_pop_longitudinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bres_pop_merged = pd.merge(bres_proc,lad_pop,left_on=['LAD','year'],right_on=['geography_name','date_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bres_pop_merged['sector_share_pop'] = bres_pop_merged['value']/bres_pop_merged['obs_value']\n",
    "\n",
    "journ_share_pop = bres_pop_merged.pivot_table(index=['LAD','year'],columns='sector_aggregated',values='sector_share_pop')['journalism'].reset_index(\n",
    "    drop=False).pivot_table(index='LAD',columns='year',values='journalism').sort_values(2017,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (1000*journ_share_pop[[2009,2017]]).plot(figsize=(20,5))\n",
    "\n",
    "#ax.set_xticks(np.arange(len(journ_share_pop.index)))\n",
    "#ax.set_xticklabels(journ_share_pop.index,rotation=90,size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
